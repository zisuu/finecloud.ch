<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>Finecloud</title>
    <link href="https://www.finecloud.ch/feed.xml" rel="self" />
    <link href="https://www.finecloud.ch" />
    <updated>2024-08-05T20:11:49+02:00</updated>
    <author>
        <name>Finecloud</name>
    </author>
    <id>https://www.finecloud.ch</id>

    <entry>
        <title>GitHub classic vs. fine-grained Personal Access Tokens</title>
        <author>
            <name>Finecloud</name>
        </author>
        <link href="https://www.finecloud.ch/github-classic-vs-fine-grained-personal-access-tokens.html"/>
        <id>https://www.finecloud.ch/github-classic-vs-fine-grained-personal-access-tokens.html</id>
            <category term="security"/>
            <category term="github"/>
            <category term="devops"/>

        <updated>2024-07-31T17:32:59+02:00</updated>
            <summary>
                <![CDATA[
                    What are PATs? Personal access tokens are an alternative to using passwords for authentication to GitHub when using the GitHub API or the command line. Personal access tokens are intended to access GitHub resources on your behalf. To access resources on behalf of an organization,&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                
  <div class="post__toc">
    <h3>Contents</h3>
    <ul>
      <li><a href="#what-are-pats">What are PATs?</a></li><li><a href="#what-are-classic-pats">What are classic PATs?</a></li><li><a href="#what-are-fine-grained-pats">What are fine-grained PATs?</a></li><li><a href="#what-happens-when-the-user-who-generated-them-becomes-inactive-and-loses-access-to-the-resource">What happens when the user who generated them becomes inactive and loses access to the resource?</a></li><li><a href="#what-are-the-risks-to-consider-with-pats">What are the risks to consider with PATs?</a></li><li><a href="#can-a-classic-pat-owned-by-a-regular-gh-user-be-used-to-access-and-manipulate-a-gh-internal-repo-from-externally-without-any-other-auth-requirement">Can a classic PAT (owned by a regular GH user) be used to access and manipulate a GH internal Repo from externally without any other auth requirement?</a></li><li><a href="#can-a-fine-grained-pat-owned-by-a-regular-gh-user-be-used-to-access-and-manipulate-a-gh-internal-repo-from-externally-without-any-other-auth-requirement">Can a fine-grained PAT (owned by a regular GH User) be used to access and manipulate a GH internal Repo from externally without any other auth requirement?</a></li><li><a href="#can-a-fine-grained-pat-owned-by-the-org-be-used-to-access-and-manipulate-a-gh-internal-repo-externally-without-any-other-auth-requirement">Can a fine-grained PAT (owned by the Org) be used to access and manipulate a GH internal Repo externally without any other auth requirement?</a></li><li><a href="#how-can-org-admins-restrict-classic-pats-access-to-the-org">How can Org. Admins restrict classic PATs access to the Org?</a></li><li><a href="#how-can-you-manage-classic-pats-of-the-users-as-an-org-adminlessbrgreater">How can you manage classic PATs of the Users as an Org Admin?<br></a></li><li><a href="#how-can-i-restrict-org-access-of-fine-grained-pats-as-an-org-admin">How can I restrict Org access of fine-grained PATs as an Org Admin?</a></li><li><a href="#how-can-i-manage-fine-grained-pats-of-the-users-as-an-org-admin">How can I manage fine-grained PATs of the Users as an Org Admin?</a></li><li><a href="#overview-of-classic-vs-fine-grained-pats">Overview of Classic vs. Fine-grained PATs</a></li>
    </ul>
  </div>
  

    <h2 id="what-are-pats">
      What are PATs?
    </h2>

  <p>
    Personal access tokens are an alternative to using passwords for authentication to GitHub when using the <a href="https://docs.github.com/en/rest/overview/authenticating-to-the-rest-api" target="_blank">GitHub API</a> or the <a href="https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#using-a-personal-access-token-on-the-command-line" target="_blank">command line</a>.<br><br>Personal access tokens are intended to access GitHub resources on your behalf. <strong>To access resources on behalf of an organization, or for long-lived integrations, you should use a GitHub App. For more information, see "<a href="https://docs.github.com/en/apps/creating-github-apps/setting-up-a-github-app/about-creating-github-apps" target="_blank">About creating GitHub Apps</a>"</strong><br><br><strong>GitHub recommends that you use fine-grained personal access tokens instead of personal access tokens (classic) whenever possible. (<a href="https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#types-of-personal-access-tokens" target="_blank">source</a>)</strong>
  </p>

    <h2 id="what-are-classic-pats">
      What are classic PATs?
    </h2>

  <p>
    Personal access tokens (classic) are less secure. <strong>However, some features currently will only work with personal access tokens (classic):</strong>
  </p>

  <ul>
    <li><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: 1em; font-weight: var(--font-weight-normal);">Only personal access tokens (classic) have write access for public repositories that are not owned by you or an organization that you are not a member of.</span><br></li><li>Outside collaborators can only use personal access tokens (classic) to access organization repositories that they are a collaborator on.</li><li>A few REST API endpoints are only available with a personal access tokens (classic). To check whether an endpoint also supports fine-grained personal access tokens, see the documentation for that endpoint, or see "<a href="https://docs.github.com/en/rest/overview/endpoints-available-for-fine-grained-personal-access-tokens" target="_blank">Endpoints available for fine-grained personal access tokens</a>".</li>
  </ul>

  <p>
    If you choose to use a personal access token (classic), keep in mind that it will grant access to all repositories within the organizations that you have access to, as well as all personal repositories in your personal account.<br><br>As a security precaution, GitHub automatically removes personal access tokens that haven't been used in a year. To provide additional security, we highly recommend adding an expiration to your personal access tokens.
  </p>

    <h2 id="what-are-fine-grained-pats">
      What are fine-grained PATs?
    </h2>

  <p>
    Fine-grained personal access tokens have several security advantages over personal access tokens (classic):
  </p>

  <ul>
    <li>Each token can only access resources owned by a single user or organization.</li><li>Each token can only access specific repositories.</li><li>Each token is granted specific permissions, which offer more control than the scopes granted to personal access tokens (classic).</li><li>Each token must have an expiration date.</li><li>Organization owners can require approval for any fine-grained personal access tokens that can access resources in the organization.</li>
  </ul>

    <h2 id="what-happens-when-the-user-who-generated-them-becomes-inactive-and-loses-access-to-the-resource">
      What happens when the user who generated them becomes inactive and loses access to the resource?
    </h2>

  <p>
    Both fine-grained personal access tokens and personal access tokens (classic) are tied to the user who generated them and will become inactive if the user loses access to the resource.
  </p>

    <h2 id="what-are-the-risks-to-consider-with-pats">
      What are the risks to consider with PATs?
    </h2>

  <ul>
    <li>PATs could be leaked if they get into the wrong hands and are used externally</li><li>    Org. Admins don't see users classic PATs with Org. access</li>
  </ul>

    <h2 id="can-a-classic-pat-owned-by-a-regular-gh-user-be-used-to-access-and-manipulate-a-gh-internal-repo-from-externally-without-any-other-auth-requirement">
      Can a classic PAT (owned by a regular GH user) be used to access and manipulate a GH internal Repo from externally without any other auth requirement?
    </h2>

  <p>
    Yes. You need to create a classic PAT, select all permissions and then try to access an internal GitHub Test Repo from an external Computer without any VPN/internal access or SSO auth session. You will be able to pull all the Repo content and write everywhere where the owner of the PAT has access, e.g., create Issues, close PRs, etc.
  </p>

    <h2 id="can-a-fine-grained-pat-owned-by-a-regular-gh-user-be-used-to-access-and-manipulate-a-gh-internal-repo-from-externally-without-any-other-auth-requirement">
      Can a fine-grained PAT (owned by a regular GH User) be used to access and manipulate a GH internal Repo from externally without any other auth requirement?
    </h2>

  <p>
    Per default, user-owned fine-grained PATs have only read access to public github.com repos. To select access to an internal GH Repo, the Organization must own the PAT, which is only possible if the token request gets approved by an Org. Admin.
  </p>

    <h2 id="can-a-fine-grained-pat-owned-by-the-org-be-used-to-access-and-manipulate-a-gh-internal-repo-externally-without-any-other-auth-requirement">
      Can a fine-grained PAT (owned by the Org) be used to access and manipulate a GH internal Repo externally without any other auth requirement?
    </h2>

  <p>
    Once the requested PAT got approved by an Org Admin, yes, this is possible. But the token has a maximum lifetime of 1 year. If the user of the PAT changes something on the permissions, the review workflow is again triggered, and an Org Admin must approve the permission change before it is applied.
  </p>

    <h2 id="how-can-org-admins-restrict-classic-pats-access-to-the-org">
      How can Org. Admins restrict classic PATs access to the Org?
    </h2>

  <p>
    Organization owners/admins can prevent personal access tokens (classic) from accessing resources owned by the organization. Personal access tokens (classic) can still read public resources within the organization.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://www.finecloud.ch/media/posts/104/gh-token.png" height="258" width="934" alt=""  sizes="100vw" srcset="https://www.finecloud.ch/media/posts/104/responsive/gh-token-xs.png 300w ,https://www.finecloud.ch/media/posts/104/responsive/gh-token-sm.png 480w ,https://www.finecloud.ch/media/posts/104/responsive/gh-token-md.png 768w ,https://www.finecloud.ch/media/posts/104/responsive/gh-token-lg.png 1024w ,https://www.finecloud.ch/media/posts/104/responsive/gh-token-xl.png 1360w ,https://www.finecloud.ch/media/posts/104/responsive/gh-token-2xl.png 1600w">
      
    </figure>

  <p>
    https://github.com/organizations/&lt;your-gh-org-name&gt;/settings/personal-access-tokens
  </p>

    <h2 id="how-can-you-manage-classic-pats-of-the-users-as-an-org-adminlessbrgreater">
      How can you manage classic PATs of the Users as an Org Admin?<br>
    </h2>

  <p>
    You can't, because you don't see them. Only the User who created the classic PAT can see and manage it.
  </p>

    <h2 id="how-can-i-restrict-org-access-of-fine-grained-pats-as-an-org-admin">
      How can I restrict Org access of fine-grained PATs as an Org Admin?
    </h2>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://www.finecloud.ch/media/posts/104/gh-token-2.png" height="264" width="928" alt=""  sizes="100vw" srcset="https://www.finecloud.ch/media/posts/104/responsive/gh-token-2-xs.png 300w ,https://www.finecloud.ch/media/posts/104/responsive/gh-token-2-sm.png 480w ,https://www.finecloud.ch/media/posts/104/responsive/gh-token-2-md.png 768w ,https://www.finecloud.ch/media/posts/104/responsive/gh-token-2-lg.png 1024w ,https://www.finecloud.ch/media/posts/104/responsive/gh-token-2-xl.png 1360w ,https://www.finecloud.ch/media/posts/104/responsive/gh-token-2-2xl.png 1600w">
      
    </figure>

  <p>
    https://github.com/organizations/&lt;your-gh-org-name&gt;<your-gh-org-name>/settings/personal-access-tokens</your-gh-org-name><br>
  </p>

    <h2 id="how-can-i-manage-fine-grained-pats-of-the-users-as-an-org-admin">
      How can I manage fine-grained PATs of the Users as an Org Admin?
    </h2>

  <p>
    You can't manage/see the fine-grained PATs the User Account owns. Once the Org owns the PAT, you can see and manage them.<br><br>If the owner is set to the User, the PAT is invisible/unable to manage, but this also means that token only has access to public repos:
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://www.finecloud.ch/media/posts/104/SCR-20240805-qyyf.png" height="429" width="996" alt=""  sizes="100vw" srcset="https://www.finecloud.ch/media/posts/104/responsive/SCR-20240805-qyyf-xs.png 300w ,https://www.finecloud.ch/media/posts/104/responsive/SCR-20240805-qyyf-sm.png 480w ,https://www.finecloud.ch/media/posts/104/responsive/SCR-20240805-qyyf-md.png 768w ,https://www.finecloud.ch/media/posts/104/responsive/SCR-20240805-qyyf-lg.png 1024w ,https://www.finecloud.ch/media/posts/104/responsive/SCR-20240805-qyyf-xl.png 1360w ,https://www.finecloud.ch/media/posts/104/responsive/SCR-20240805-qyyf-2xl.png 1600w">
      
    </figure>

  <p>
    If you select the Organization as the Resource owner of the PAT instead, a fine-grained personal access token request will be made, which needs to be approved by an Org. Admin before it can be used.
  </p>

  <p>
    An overview of all Org. owned PATs can be found under this URL; if you are an Org. Admin:&nbsp;https://github.com/organizations/&lt;your-gh-org-name&gt;/settings/personal-access-tokens/active
  </p>

    <h2 id="overview-of-classic-vs-fine-grained-pats">
      Overview of Classic vs. Fine-grained PATs
    </h2>
<div><style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-wp8o{text-align:center;vertical-align:top}
.tg .tg-73oq{text-align:left;vertical-align:top}
</style>
<table class="tg"><thead>
  <tr>
    <th class="tg-73oq"></th>
    <th class="tg-wp8o">Classic PAT</th>
    <th class="tg-wp8o">Fine-grained PAT</th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-73oq">write access for public repositories that are not owned by you or an organization that you are not a member of</td>
    <td class="tg-wp8o">✅</td>
    <td class="tg-wp8o">❌</td>
  </tr>
  <tr>
    <td class="tg-73oq">Outside collaborators can access organization repositories that they are a collaborator on.</td>
    <td class="tg-wp8o">✅</td>
    <td class="tg-wp8o">❌</td>
  </tr>
  <tr>
    <td class="tg-73oq">Can be used with all REST API endpoints</td>
    <td class="tg-wp8o">✅</td>
    <td class="tg-wp8o">❌</td>
  </tr>
  <tr>
    <td class="tg-73oq">Feature release status</td>
    <td class="tg-wp8o">stable</td>
    <td class="tg-wp8o">beta<br></td>
  </tr>
  <tr>
    <td class="tg-73oq">Tokens must have an expiration date</td>
    <td class="tg-wp8o">❌</td>
    <td class="tg-wp8o">✅</td>
  </tr>
  <tr>
    <td class="tg-73oq">Token can inherit all permissions of the User, incl. all repositories <br>within the organizations that the user has access to without any <br>approval/reivew</td>
    <td class="tg-wp8o">✅</td>
    <td class="tg-wp8o">❌</td>
  </tr>
  <tr>
    <td class="tg-73oq">Token permissions can be defined set on repository level (fine-grained)</td>
    <td class="tg-wp8o">❌</td>
    <td class="tg-wp8o">✅</td>
  </tr>
  <tr>
    <td class="tg-73oq">Organization owners can prevent token from accessing resources owned by the organization</td>
    <td class="tg-wp8o">✅</td>
    <td class="tg-wp8o">✅</td>
  </tr>
  <tr>
    <td class="tg-73oq">Organization owners can require approval for each token that can access <br>the organization (e.g. internal Repo) from externally without any other <br>auth requirement</td>
    <td class="tg-wp8o">❌</td>
    <td class="tg-wp8o">✅</td>
  </tr>
</tbody></table>
</div>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Third-party GitHub Actions</title>
        <author>
            <name>Finecloud</name>
        </author>
        <link href="https://www.finecloud.ch/third-party-github-actions.html"/>
        <id>https://www.finecloud.ch/third-party-github-actions.html</id>
            <category term="security"/>
            <category term="github"/>

        <updated>2024-04-08T20:50:51+02:00</updated>
            <summary>
                <![CDATA[
                    Today I came across these steps to guide our decision-making process, before using a 3rd Part GitHub Action:
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                
  <p>
    Today I came across these steps to guide our decision-making process, before using a 3rd Part GitHub Action:
  </p>

  <ol>
    <li><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: 1em; font-weight: var(--font-weight-normal);">For simple tasks, avoid external GitHub Actions because the risk might outweigh the value. Maybe a simple curl could to it as well? 😉</span><br></li><li><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: 1em; font-weight: var(--font-weight-normal);">Use GitHub Actions from Verified Creators because they follow a strict security review process.</span><br></li><li><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: 1em; font-weight: var(--font-weight-normal);">Use the latest version of a GitHub Action because it might contain security fixes.</span><br></li><li><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: 1em; font-weight: var(--font-weight-normal);">Think about GitHub Actions like dependencies: they need to be maintained and updated. Dependabot or Renovate can help here.</span><br></li><li><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: 1em; font-weight: var(--font-weight-normal);">Think about disabling or limiting GitHub Actions for your organization(s) in Settings.</span><br></li><li><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: 1em; font-weight: var(--font-weight-normal);">Have a PR process with multiple reviewers to avoid adding a malicious GitHub Action.</span><br></li>
  </ol>

  <p>
    
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>GitHub Codespace</title>
        <author>
            <name>Finecloud</name>
        </author>
        <link href="https://www.finecloud.ch/github-codespace.html"/>
        <id>https://www.finecloud.ch/github-codespace.html</id>
            <category term="software development"/>
            <category term="github"/>
            <category term="git"/>
            <category term="codespace"/>

        <updated>2024-04-05T21:02:42+02:00</updated>
            <summary>
                <![CDATA[
                    What is a Codespace? A codespace is a development environment that's hosted in the cloud. You can customize your project for GitHub Codespaces by committing configuration files to your repository (also known as configuration-as-code), which creates a repeatable codespace configuration for all users of your&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                
    <h2 id="what-is-a-codespace">
      What is a Codespace?
    </h2>

  <p>
    A codespace is a development environment that's hosted in the cloud. You can customize your project for GitHub Codespaces by committing configuration files to your repository (also known as configuration-as-code), which creates a repeatable codespace configuration for all users of your project. Each codespace you create is hosted by GitHub in a Docker container that runs on a virtual machine. You can choose the type of machine you want to use depending on the resources you need.
  </p>

    <h2 id="how-can-i-create-a-codespace">
      How can I create a Codespace?
    </h2>

  <p>
    There are four ways to create a Codespace:&nbsp;
  </p>

  <ul>
    <li>From a GitHub template or any template repository on GitHub.com to start a new project.</li><li>    From a branch in your repository for new feature work.</li><li>    From an open pull request to explore work-in-progress.</li><li>    From a commit in a repository's history to investigate a bug at a specific point in time.</li>
  </ul>

  <p>
    The easiest way to start a Codespace is by clicking the green Code button and then choosing open in a Codespace:
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://www.finecloud.ch/media/posts/102/SCR-20240405-stqz.png" height="892" width="942" alt=""  sizes="100vw" srcset="https://www.finecloud.ch/media/posts/102/responsive/SCR-20240405-stqz-xs.png 300w ,https://www.finecloud.ch/media/posts/102/responsive/SCR-20240405-stqz-sm.png 480w ,https://www.finecloud.ch/media/posts/102/responsive/SCR-20240405-stqz-md.png 768w ,https://www.finecloud.ch/media/posts/102/responsive/SCR-20240405-stqz-lg.png 1024w ,https://www.finecloud.ch/media/posts/102/responsive/SCR-20240405-stqz-xl.png 1360w ,https://www.finecloud.ch/media/posts/102/responsive/SCR-20240405-stqz-2xl.png 1600w">
      
    </figure>

    <h4 id="what-happens-when-i-create-a-codespace">
      What happens when I create a Codespace?
    </h4>

  <p>
    When you create a GitHub Codespace, four processes occur:
  </p>

  <ol>
    <li>VM and storage are assigned to your Codespace.</li><li>    A container is created.</li><li>    A connection to the Codespace is made.</li><li>    A post-creation setup is made.</li>
  </ol>

    <h2 id="what-you-can-customize">
      What you can customize
    </h2>

  <p>
    There are many ways you can customize your Codespace. Let's review each one.
  </p>

  <ul>
    <li>Settings Sync: You can synchronize your Visual Studio Code (VS Code) settings between the desktop application and the VS Code web client.</li><li>Dotfiles: You can use a dotfiles repository to specify scripts, shell preferences, and other configurations.<br></li><li>    Rename a Codespace: When you create a Codespace, it's assigned an autogenerated display name. If you have multiple Codespaces, the display name helps you to differentiate between Codespaces. You can change the display name for your Codespace.<br></li><li>    Change your shell: You can change your shell in a Codespace to keep the setup you're used to. When you're working in a Codespace, you can open a new terminal window with a shell of your choice, change your default shell for new terminal windows, or install a new shell. You can also use dotfiles to configure your shell.<br></li><li>    Change the machine type: You can change the type of machine that's running your Codespace, so that you're using resources appropriate for the work you're doing.<br></li><li>    Set the default editor: You can set your default editor for Codespaces in your personal settings page. Set your editor preference so that when you create a Codespace or open an existing Codespace, it opens to your default editor.<br>- Visual Studio Code (desktop application)<br>- Visual Studio Code (web client application)<br>- JetBrains Gateway - for opening Codespaces in a JetBrains IDE<br>- JupyterLab - the web interface for Project Jupyter<br></li><li>    Set the default region: You can set your default region in the GitHub Codespaces profile settings page to personalize where your data is held.<br></li><li>    Set the timeout: A Codespace will stop running after a period of inactivity. By default this period is 30 minutes, but you can specify a longer or shorter default timeout period in your personal settings on GitHub. The updated setting applies to any new Codespaces you create, or to existing Codespaces the next time you start them.</li><li>Configure automatic deletion: Inactive Codespaces are automatically deleted. You can choose how long your stopped Codespaces are retained, up to a maximum of 30 days.<br></li>
  </ul>

    <h2 id="difference-betweenandnbspcodespaces-and-githubdev-editor">
      Difference between&nbsp;Codespaces and GitHub.dev editor
    </h2>

    <h4 id="when-should-i-use-github-codespaces-and-when-should-i-use-githubdev">
      when should I use GitHub Codespaces and when should I use GitHub.dev?
    </h4>

  <p>
    GitHub.dev is a good fit if you only want to navigate some files and sources code repositories from GitHub, and maybe make and commit small code changes.
  </p>

  <p>
    On the other hand, if you want to run a bunch of tests with your code, or build a heavy application you better use GitHub Codespaces. It has compute associated with it so you can build your code, run your code, and have terminal access. GitHub.dev doesn't have compute in it. With GitHub Codespaces, you get the power of a personal Virtual Machine (VM) with terminal access, the same way you could use your local environment, just in the cloud.
  </p>
<div><table>
  <tr>
    <th></th>
    <th>GitHub.dev</th>
    <th>GitHub Codespaces</th>
  </tr>
  <tr>
    <td>Cost</td>
    <td>Free</td>
    <td>Free monthly quota of usage for personal accounts</td>
  </tr>
  <tr>
    <td>Availability</td>
    <td>Available to everyone on GitHub.com</td>
    <td>Available to everyone on GitHub.com</td>
  </tr>
  <tr>
    <td>Startup</td>
    <td>GitHub.dev opens instantly with a key-press and you can start using it right away without having to wait for configuration or installation</td>
    <td>When you create or resume a Codespace, the Codespace is assigned a VM, and the container is configured based on the contents of a devcontainer.json file. This setup takes a few minutes to create the development environment.</td>
  </tr>
  <tr>
    <td>Compute</td>
    <td>There's no associated compute, so you can't build and run your code or use the integrated terminal.</td>
    <td>With GitHub Codespaces, you get the power of a dedicated VM to run and debug your application.</td>
  </tr>
  <tr>
    <td>Terminal access</td>
    <td>None</td>
    <td>GitHub Codespaces provides a common set of tools by default, meaning that you can use the Terminal exactly as you would in your local environment.</td>
  </tr>
  <tr>
    <td>Extensions</td>
    <td>Only a subset of extensions that can run on the web appear in the extensions view and can be installed</td>
    <td>With GitHub Codespaces, you can use most extensions from the Visual Studio Code Marketplace.</td>
  </tr>
</table>
</div>

    <h2 id="using-codespace-images">
      Using Codespace images
    </h2>

  <p>
    One of the biggest benefits of codespaces is that Github ships images with the most recent and common tools you need to develop. Have a look at this Repository:&nbsp;<a href="https://github.com/devcontainers/images/tree/main/src/universal" target="_blank" rel="nofollow noopener">https://github.com/devcontainers/images/tree/main/src/universal</a>
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Awk and Sed</title>
        <author>
            <name>Finecloud</name>
        </author>
        <link href="https://www.finecloud.ch/awk-and-sed.html"/>
        <id>https://www.finecloud.ch/awk-and-sed.html</id>
            <category term="shell"/>
            <category term="sed"/>
            <category term="linux"/>
            <category term="bash"/>
            <category term="awk"/>

        <updated>2024-04-05T20:57:46+02:00</updated>
            <summary>
                <![CDATA[
                    awk and sed are text manipulation programs. You can use them for example to replace strings: echo image.jpg | sed 's/\.jpg/.png/' image.png or change order of strings: echo "hello world" | awk '{print $2, $1}' world hello awk and sed are harder to learn than&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                
  <p>
    awk and sed are text manipulation programs. You can use them for example to replace strings:
  </p>
<pre class=" language-bash"><code>echo image.jpg | sed 's/\.jpg/.png/'
image.png
</code></pre>

  <p>
    or change order of strings:<br>
  </p>
<pre class=" language-bash"><code>echo "hello world" | awk '{print $2, $1}'
world hello
</code></pre>

  <p>
    awk and sed are harder to learn than some other basic linux cli tools because they contain their own small programming language.<br>
  </p>

    <h2 id="awk-basics">
      awk basics
    </h2>

  <p>
    awk transforms lines of text (stdin) into any other text, using a set of instructions (called the awk program):
  </p>
<pre class=" language-bash"><code>awk program input-files
</code></pre>

  <p>
    a awk program can also contain one or more `actions`, for example calculating values or printing text. These `actions` run only when an input matches a `pattern`:<br>`pattern {action}`
  </p>

  <p>
    typical patterns include:
  </p>

  <ul>
    <li>`BEGIN` runs once before the awk processes any input</li><li>`END` run once after awk has processed all the input</li><li>A Regex surrounded by forward slashes, example: `/^[A-Z]/`</li><li>other awk specific expressions, example: `FNR&gt;5` tells awk to skip the first five lines of input</li>
  </ul>

  <p>
    <br>A `pattern` with no `action` runs the default action `{print}` .<br>awk can also perform calculations such as these:
  </p>
<pre class=" language-bash"><code>seq 1 100 | awk '{s+=$1} END {print s}'
5050</code></pre>

  <p>
    (sum numbers 1 to 100)
  </p>

    <h2 id="sed-basics">
      sed basics
    </h2>

  <p>
    sed like awk can be used to transform text from stdin to any other text using instructions called `sed scripts`:
  </p>
<pre class=" language-bash"><code>sed script input-files</code></pre>

  <p>
    the most common use case for sed is text replacement, like in this example:<br>
  </p>
<pre class=" language-bash"><code>echo "Windows eats Linux for breakfast" | sed s/Windows/Linux/g | sed s/Linux/Windows/2
</code></pre>

  <p>
    
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>How to do a code review</title>
        <author>
            <name>Finecloud</name>
        </author>
        <link href="https://www.finecloud.ch/how-to-do-a-code-review.html"/>
        <id>https://www.finecloud.ch/how-to-do-a-code-review.html</id>
            <category term="software development"/>
            <category term="devops"/>

        <updated>2024-03-18T08:44:04+01:00</updated>
            <summary>
                <![CDATA[
                    This Blog post is my personal summary of&nbsp;Googles code review process Make sure to review every line of code you’ve been asked to review, look at the context, make sure you’re improving code health, and compliment developers on good things that they do. Look at&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                
  <p>
    This Blog post is my personal summary of&nbsp;<a href="https://google.github.io/eng-practices/review/" target="_blank" rel="nofollow noopener">Googles code review process</a>
  </p>

  <p>
    
  </p>

  <div class="post__toc">
    <h3>Table of contents</h3>
    <ul>
      <li><a href="#summary-of-what-you-should-look-at">Summary of what you should look at</a></li><li><a href="#the-standard-of-code-review">The Standard of Code Review</a></li><li><a href="#navigate-a-pull-request-pr-in-review">Navigate a Pull-Request (PR) in review</a><ul><li><a href="#1-take-a-broad-view-of-the-change">1. Take a broad view of the change</a></li><li><a href="#2-examine-the-main-parts-of-the-pr">2. Examine the main parts of the PR</a></li><li><a href="#3andnbsplook-through-the-rest-of-the-pr-in-an-appropriate-sequence">3.&nbsp;Look through the rest of the PR in an appropriate sequence</a></li></ul></li><li><a href="#speed-of-code-reviews">Speed of Code Reviews</a><ul><li><a href="#approve-with-comments">Approve with Comments</a></li></ul></li><li><a href="#how-to-write-comments">How to write comments</a></li>
    </ul>
  </div>
  

    <h2 id="summary-of-what-you-should-look-at">
      Summary of what you should look at
    </h2>

  <ul>
    <li><strong>Design</strong>: Is the code well-designed and appropriate for your system?<br></li><li>    <strong>Functionality</strong>: Does the code behave as the author likely intended? Is the way the code behaves good for its users?</li><li><strong>Complexity</strong>: Could the code be made simpler? Would another developer be able to easily understand and use this code when they come across it in the future?</li><li><strong>Tests</strong>: Does the code have correct and well-designed automated tests?<br></li><li>    <strong>Naming</strong>: Did the developer choose clear names for variables, classes, methods, etc.?</li><li><strong>Comments</strong>: Are the comments clear and useful?</li><li><strong>Style</strong>: Does the code follow our style guides?</li><li><strong>Documentation</strong>: Did the developer also update relevant documentation?<br></li>
  </ul>

  <p>
    Make sure to review <strong>every line</strong> of code you’ve been asked to review, look at the <strong>context</strong>, make sure you’re <strong>improving code health</strong>, and compliment developers on <strong>g</strong><strong>ood things</strong> that they do.
  </p>

    <h2 id="the-standard-of-code-review">
      The Standard of Code Review
    </h2>

  <ul>
    <li>You need to balance the tradeoff between making progress, by letting a change go into your code base and not decreasing overall code health and quality.&nbsp;</li><li><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: 1em; font-weight: var(--font-weight-normal);">does the Pull-Request improve the maintainability, readability, and understandability?</span><br></li><li><strong>In general you should try to approve a Pull-Request (PR) once it is in a state where it definitely improves the overall code health, even if the PR isn’t perfect.</strong></li><li>Reviewers should not require the author to polish every tiny piece of a CL before granting approval. Rather, the reviewer should balance out the need to make forward progress compared to the importance of the changes they are suggesting.&nbsp;</li><li>Instead of seeking perfection, what a reviewer should seek is continuous improvement.</li><li>If you add a comment on a PR to mention something could be better, but it’s optional and not very important,&nbsp;prefix it with something like “Nit: “ to let the author know that it’s just a point of polish that they could choose to ignore.</li>
  </ul>

    <h2 id="navigate-a-pull-request-pr-in-review">
      Navigate a Pull-Request (PR) in review
    </h2>

    <h3 id="1-take-a-broad-view-of-the-change">
      1. Take a broad view of the change
    </h3>

  <p>
    Look at the PR description and what the PR does in general. Does this change even make sense? If this change shouldn’t have happened in the first place, please respond immediately with an explanation of why the change should not be happening. When you reject a change like this, it’s also a good idea to suggest to the developer what they should have done instead.
  </p>

    <h3 id="2-examine-the-main-parts-of-the-pr">
      2. Examine the main parts of the PR
    </h3>

  <p>
    Find the file or files that are the “main” part of this PR. Often, there is one file that has the largest number of logical changes, and it’s the major piece of the PR. Look at these major parts first. This helps give context to all of the smaller parts of the PR, and generally accelerates doing the code review.&nbsp;
  </p>

    <h3 id="3andnbsplook-through-the-rest-of-the-pr-in-an-appropriate-sequence">
      3.&nbsp;Look through the rest of the PR in an appropriate sequence
    </h3>

  <p>
    Once you’ve confirmed there are no major design problems with the CL as a whole, try to figure out a logical sequence to look through the files while also making sure you don’t miss reviewing any file.
  </p>

    <h2 id="speed-of-code-reviews">
      Speed of Code Reviews
    </h2>

  <p>
    Why is it so important that you send comments out immediately, especially if you see major design problems within a PR?
  </p>

  <ul>
    <li>Developers often open a PR and then immediately start new work based on that PR (e.g. feature branch) while they wait for review. If there are major design problems in the PR you’re reviewing, they’re also going to have to re-work their later PR. You want to catch them before they’ve done too much extra work on top of the problematic design.</li><li>Major design changes take longer to do than small changes. Developers nearly all have deadlines; in order to make those deadlines and still have quality code in the codebase, the developer needs to start on any major re-work of the PR as soon as possible.<br></li>
  </ul>

  <p>
    When code reviews are slow, several things happen:
  </p>

  <ul>
    <li>The velocity of the team as a whole is decreased.</li><li>Developers start to protest the code review process.&nbsp;Most complaints about the code review process are actually resolved by making the process faster.</li><li>Code health can be impacted.<br></li>
  </ul>

  <p>
    How fast should a code review be?&nbsp;
  </p>

  <ul>
    <li>If you are not in the middle of a focused task, <strong>you should do a code review shortly after it comes in</strong>.</li><li><strong>One business day is the maximum time it should take to respond</strong> to a code review request (i.e., first thing the next morning).</li>
  </ul>

  <p>
    <strong>If you are in the middle of a focused task, such as writing code, don’t interrupt yourself to do a code review</strong>. Research has shown that it can take a long time for a developer to get back into a smooth flow of development after being interrupted. So interrupting yourself while coding is actually more expensive to the team than making another developer wait a bit for a code review.
  </p>

    <h3 id="approve-with-comments">
      Approve with Comments
    </h3>

  <p>
    In order to speed up code reviews, there are certain situations in which a reviewer should give the Approval even though they are also leaving unresolved comments on the PR. This is done when either:
  </p>

  <ul>
    <li>The reviewer is confident that the developer will appropriately address all the reviewer’s remaining comments.</li><li>    The remaining changes are minor and don’t have to be done by the developer.</li>
  </ul>

    <h2 id="how-to-write-comments">
      How to write comments
    </h2>

  <ul>
    <li>Be kind.</li><li>Explain your reasoning (why?).</li><li>Balance giving explicit directions with just pointing out problems and letting the developer decide -&gt; In general it is the developer’s responsibility to fix a PR, not the reviewer’s.</li><li>Encourage developers to simplify code or add code comments instead of just explaining the complexity to you.<br></li>
  </ul>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Building a GraphQL service</title>
        <author>
            <name>Finecloud</name>
        </author>
        <link href="https://www.finecloud.ch/building-a-graphql-service.html"/>
        <id>https://www.finecloud.ch/building-a-graphql-service.html</id>
            <category term="spring-framework"/>
            <category term="spring"/>
            <category term="software development"/>
            <category term="java"/>
            <category term="graphql"/>
            <category term="api"/>

        <updated>2024-01-22T21:19:17+01:00</updated>
            <summary>
                <![CDATA[
                    Let's build a GraphQL Spring Application that will accept GraphQL requests at http://localhost:8080/graphql. First let's navigate to https://start.spring.io. This service pulls in all the dependencies you need for an application and does most of the setup for you. GraphQL is a query language to retrieve&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                
  <div class="post__toc">
    <h3>Table of contents</h3>
    <ul>
      <li><a href="#preparation">Preparation</a></li><li><a href="#a-very-short-introduction-to-graphql">A very short introduction to GraphQL</a></li><li><a href="#our-example-api-getting-book-details">Our example API: getting book details</a></li><li><a href="#schema">Schema</a></li><li><a href="#source-of-the-data">Source of the data</a></li><li><a href="#create-the-book-and-author-data-sources">Create the Book and Author data sources</a></li><li><a href="#adding-code-to-fetch-data">Adding code to fetch data</a></li><li><a href="#running-our-first-query">Running our first query</a><ul><li><a href="#enable-the-graphiql-playground">Enable the GraphiQL Playground</a></li><li><a href="#boot-the-application">Boot the application</a></li><li><a href="#run-the-query">Run the query</a></li></ul></li><li><a href="#testing">Testing</a></li>
    </ul>
  </div>
  

  <p>
    Let's build a GraphQL Spring Application that will accept GraphQL requests at http://localhost:8080/graphql.
  </p>

    <h2 id="preparation">
      Preparation
    </h2>

  <p>
    First let's navigate to https://start.spring.io. This service pulls in all the dependencies you need for an application and does most of the setup for you.<br>
  </p>

  <ul>
    <li>Choose Maven and chose Java.Click&nbsp;</li><li>Dependencies and select Spring for GraphQL and Spring Web.</li><li><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: 1em; font-weight: var(--font-weight-normal);">Click Generate.</span><br></li><li><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: 1em; font-weight: var(--font-weight-normal);">Download the resulting ZIP file, which is an archive of a GraphQL application that is configured with your choices.</span><br></li><li>Unzip and open the Folder with your favorite IDE (IntelliJ recommended)</li>
  </ul>

    <h2 id="a-very-short-introduction-to-graphql">
      A very short introduction to GraphQL
    </h2>

  <p>
    GraphQL is a query language to retrieve data from a server. It is an alternative to REST, SOAP, or gRPC. In the next Part we will query the details for a specific book from an online store backend.<br><br>This is an example request you can send to a GraphQL server to retrieve book details:
  </p>
<pre class=" language-graphql"><code>query bookDetails {
  bookById(id: "book-1") {
    id
    name
    pageCount
    author {
      firstName
      lastName
    }
  }
}</code></pre>

  <p>
    This GraphQL request says:
  </p>

  <ul>
    <li>perform a query for a book with id "book-1"</li><li><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: 1em; font-weight: var(--font-weight-normal);">for the book, return id, name, pageCount and author</span><br></li><li><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: 1em; font-weight: var(--font-weight-normal);">for the author, return firstName and lastName</span><br></li>
  </ul>

  <p>
    The response is in JSON. For example:<br>
  </p>
<pre class=" language-json"><code>{
  "bookById": {
    "id":"book-1",
    "name":"Effective Java",
    "pageCount":416,
    "author": {
      "firstName":"Joshua",
      "lastName":"Bloch"
    }
  }
}</code></pre>

  <p>
    An important feature of GraphQL is that it defines a schema language, and that it is statically typed. The server knows exactly what types of objects requests can query and what fields those objects contain. Furthermore, clients can introspect the server to ask for schema details.
  </p>

  <p class="msg msg--info">
    The word schema in this Post refers to a "GraphQL Schema", which is not related to other schemas like "JSON Schema" or "Database Schema".
  </p>

  <p>
    The schema for the above query is:
  </p>
<pre class=" language-graphql"><code>type Query {
    bookById(id: ID): Book
}

type Book {
    id: ID
    name: String
    pageCount: Int
    author: Author
}

type Author {
    id: ID
    firstName: String
    lastName: String
}</code></pre>

  <p>
    This Post will focus on how to implement a GraphQL server with this schema in Java.<br><br>We’ve barely scratched the surface of what’s possible with GraphQL. Further information can be found on the official GraphQL page.
  </p>

    <h2 id="our-example-api-getting-book-details">
      Our example API: getting book details
    </h2>

  <p>
    These are the main steps to create a server with Spring for GraphQL:
  </p>

  <ol>
    <li>Define a GraphQL schema</li><li>Implement the logic to fetch the actual data for a query<br></li>
  </ol>

  <p>
    Our example app will be a simple API to get details for a specific book. It is not intended to be a comprehensive API.
  </p>

    <h2 id="schema">
      Schema
    </h2>

  <p>
    In your Spring for GraphQL application prepared earlier, add a new file <em>schema.graphqls</em> to the <em>src/main/resources/graphql</em> folder with the following content:
  </p>
<pre class=" language-graphql"><code>type Query {
    bookById(id: ID): Book
}

type Book {
    id: ID
    name: String
    pageCount: Int
    author: Author
}

type Author {
    id: ID
    firstName: String
    lastName: String
}</code></pre>

  <p>
    Every GraphQL schema has a top-level <em>Query</em> type, and the fields under it are the query operations exposed by the application. Here the schema defines one query called <em>bookById</em> that returns the details of a specific book.<br><br>It also defines the types <em>Book</em> with fields<em> id, name, pageCount</em> and <em>author</em>, and the type <em>Author</em> with fields <em>firstName</em> and <em>lastName</em>.<br>
  </p>

  <p class="msg msg--info">
    The Domain Specific Language used above to describe a schema is called the Schema Definition Language or SDL. For more details, see the GraphQL documentation.
  </p>

    <h2 id="source-of-the-data">
      Source of the data
    </h2>

  <p>
    A key strength of GraphQL is that data can be sourced from anywhere. Data can come from a database, an external service, or a static in-memory list.<br><br>To simplify the demo here, book and author data will come from static lists inside their respective classes.
  </p>

    <h2 id="create-the-book-and-author-data-sources">
      Create the Book and Author data sources
    </h2>

  <p>
    Let’s now create the <em>Book</em> and <em>Author</em> classes in the main application package, right next to <em>GraphQlServerApplication</em>. Use the following as their content:
  </p>
<pre class=" language-java"><code>package com.example.graphqlserver;

import java.util.Arrays;
import java.util.List;

public record Book (String id, String name, int pageCount, String authorId) {

    private static List&lt;Book&gt; books = Arrays.asList(
            new Book("book-1", "Effective Java", 416, "author-1"),
            new Book("book-2", "Hitchhiker's Guide to the Galaxy", 208, "author-2"),
            new Book("book-3", "Down Under", 436, "author-3")
    );

    public static Book getById(String id) {
        return books.stream()
				.filter(book -&gt; book.id().equals(id))
				.findFirst()
				.orElse(null);
    }
}</code></pre>
<pre class=" language-java"><code>package com.example.graphqlserver;

import java.util.Arrays;
import java.util.List;

public record Author (String id, String firstName, String lastName) {

    private static List&lt;Author&gt; authors = Arrays.asList(
            new Author("author-1", "Joshua", "Bloch"),
            new Author("author-2", "Douglas", "Adams"),
            new Author("author-3", "Bill", "Bryson")
    );

    public static Author getById(String id) {
        return authors.stream()
				.filter(author -&gt; author.id().equals(id))
				.findFirst()
				.orElse(null);
    }
}</code></pre>

    <h2 id="adding-code-to-fetch-data">
      Adding code to fetch data
    </h2>

  <p>
    Spring for GraphQL provides an annotation-based programming model. With controller annotated methods, we can declare how to fetch the data for specific GraphQL fields.<br><br>Add the following to <em>BookController.java</em> in the main application package, next to Book and Author:
  </p>
<pre class=" language-java"><code>package com.example.graphqlserver;

import org.springframework.graphql.data.method.annotation.Argument;
import org.springframework.graphql.data.method.annotation.QueryMapping;
import org.springframework.graphql.data.method.annotation.SchemaMapping;
import org.springframework.stereotype.Controller;

@Controller
public class BookController {
    @QueryMapping
    public Book bookById(@Argument String id) {
        return Book.getById(id);
    }

    @SchemaMapping
    public Author author(Book book) {
        return Author.getById(book.authorId());
    }
}</code></pre>

  <p>
    By defining a method named <em>bookById</em> annotated with <em>@QuerMapping</em>, this controller declares how to fetch a <em>Book</em> as defined under the Query type. The query field is determined from the method name, but can also be declared on the annotation itself.
  </p>

  <p class="msg msg--info">
    Spring for GraphQL uses RuntimeWiring.Builder that registers each such controller method as a GraphQL Java graphql.schema.DataFetcher. A DataFetcher provides the logic to fetch the data for a query or for any schema field. The Spring Boot starter for GraphQL has auto-configurations that automates this registration.
  </p>

  <p>
    In the GraphQL Java engine, <em>DataFetchingEnvironment</em> provides access to a map of field-specific argument values. Use the <em>@Argument</em> annotation to have an argument bound to a target object and injected into the controller method. By default, the method parameter name is used to look up the argument, but can also be specified on the annotation itself.<br><br>This <em>bookById</em> method defines how to get a specific <em>Book</em>, but does not take care of fetching the related <em>Author</em>. If the request asks for the author information, GraphQL Java will need to fetch this field.<br><br>The <em>@SchemaMapping</em> annotation maps a handler method to a field in the GraphQL schema and declares it to be the <em>DataFetcher</em> for that field. The field name defaults to the method name, and the type name defaults to the simple class name of the source/parent object injected into the method. In this example, the field defaults to <em>author</em> and the type defaults to <em>Book</em>.<br><br>For more, see the <a href="https://docs.spring.io/spring-graphql/reference/controllers.html" target="_blank" rel="nofollow noopener">documentation for the Spring for GraphQL annotated controller feature</a>.<br><br>Now let’s run our first query.
  </p>

    <h2 id="running-our-first-query">
      Running our first query
    </h2>

    <h3 id="enable-the-graphiql-playground">
      Enable the GraphiQL Playground
    </h3>

  <p>
    GraphiQL is a useful visual interface for writing and executing queries, and much more. Enable GraphiQL by adding this config to the <em>application.properties</em> file.
  </p>
<pre class=" language-java"><code>spring.graphql.graphiql.enabled=true</code></pre>

    <h3 id="boot-the-application">
      Boot the application
    </h3>

  <p>
    Start your Spring application. Navigate to http://localhost:8080/graphiql.
  </p>

    <h3 id="run-the-query">
      Run the query
    </h3>

  <p>
    Type in the query and click the play button at the top of the window.<br>
  </p>
<pre class=" language-graphql"><code>query bookDetails {
  bookById(id: "book-1") {
    id
    name
    pageCount
    author {
      id
      firstName
      lastName
    }
  }
}</code></pre>

  <p>
    You should see a response like this:
  </p>
<pre class=" language-json"><code>{
  "data": {
    "bookById": {
      "id": "book-1",
      "name": "Effective Java",
      "pageCount": 416,
      "author": {
        "id": "author-1",
        "firstName": "Joshua",
        "lastName": "Bloch"
      }
    }
  }
}</code></pre>

  <p>
    Congratulations, you have built a GraphQL service and executed your first query! With the help of Spring for GraphQL, you were able to achieve this with only a few lines of code.
  </p>

    <h2 id="testing">
      Testing
    </h2>

  <p>
    Spring for GraphQL provides helpers for GraphQL testing in the <em>spring-graphql-test</em> artifact. We have already included this artifact as part of the project generated by Spring Initializr.<br><br>Thoroughly testing a GraphQL service requires tests with different scopes. In this tutorial, we will write a <em>@GraphQlTest</em> slice test, which focuses on a single controller. There are other helpers to assist with full end-to-end integration tests and focused server side tests. For the full details, see the Spring for GraphQL Testing documentation and Auto-configured Spring for GraphQL tests in the Spring Boot documentation.<br><br>Let’s write a controller slice test that verifies the same <em>bookDetails</em> query requested in the GraphiQL playground a few moments ago.<br><br>Add the following to a test file <em>BookControllerTests.java</em>. Save this file in a location within the <em>src/test/java/com/example/graphqlserver/</em> folder.
  </p>
<pre class=" language-java"><code>package com.example.graphqlserver;

import org.junit.jupiter.api.Test;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.autoconfigure.graphql.GraphQlTest;
import org.springframework.graphql.test.tester.GraphQlTester;

@GraphQlTest(BookController.class)
public class BookControllerTests {

    @Autowired
    private GraphQlTester graphQlTester;

    @Test
    void shouldGetFirstBook() {
        this.graphQlTester
				.documentName("bookDetails")
				.variable("id", "book-1")
                .execute()
                .path("bookById")
                .matchesJson("""
                    {
                        "id": "book-1",
                        "name": "Effective Java",
                        "pageCount": 416,
                        "author": {
                          "firstName": "Joshua",
                          "lastName": "Bloch"
                        }
                    }
                """);
    }
}
</code></pre>

  <p>
    This test refers to a GraphQL query similar to what we used in the GraphiQL Playground. It’s parameterized with an <em>$id</em> to make it reusable. Add this query in a <em>bookDetails</em>.<em>graphql</em> file located in <em>src/test/resources/graphql-test.</em>
  </p>
<pre class=" language-graphql"><code>query bookDetails($id: ID) {
    bookById(id: $id) {
        id
        name
        pageCount
        author {
            id
            firstName
            lastName
        }
    }
}</code></pre>

  <p>
    Run the test and verify that the result is identical to the GraphQL query manually requested in the GraphiQL Playground.<br><br>The <em>@GraphQlTest</em> annotation is useful for writing controller slice tests, which are focused on a single controller. <em>@GraphQlTest </em>auto-configures the Spring for GraphQL infrastructure, without any transport nor server being involved. Automatic configuration enables us to write tests faster by skipping boilerplate code. As this is a focused slice test, only a limited number of beans are scanned including <em>@Controller</em> and <em>RuntimeWiringConfigurer.</em><br><br><em>GraphQlTester</em> is a contract that declares a common workflow for testing GraphQL requests, independent of transport. In our test, we provide a document with <em>documentName</em> with the required variables, then <em>execute</em> the request. We then select a part of the response with its JSON path and assert that the JSON at this location matches the expected result.<br><br>Congratulations! In this tutorial you built a GraphQL service, ran your first query, and wrote your first GraphQL test!<br>
  </p>

  <p>
    
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Working efficient within different directories in a Linux Shell</title>
        <author>
            <name>Finecloud</name>
        </author>
        <link href="https://www.finecloud.ch/working-efficient-within-different-directories-in-a-linux-shell.html"/>
        <id>https://www.finecloud.ch/working-efficient-within-different-directories-in-a-linux-shell.html</id>
            <category term="shell"/>
            <category term="linux"/>
            <category term="bash"/>

        <updated>2023-12-10T13:52:14+01:00</updated>
            <summary>
                <![CDATA[
                    Lets Suppose you need to perform a certain kind of work in all of these directories:&nbsp; You might know about the "cd" command. But isn't it annoying if you need to use and retype a lot of Commands, like for example: $ cd ~/Work/Projects/Web/src $&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                
  <p>
    
  </p>

  <div class="post__toc">
    <h3>TOC</h3>
    <ul>
      <ul><li><a href="#directory-stack">Directory stack</a></li><li><a href="#push-a-directory-onto-the-stack">Push a directory onto the stack</a></li><li><a href="#view-a-directory-stack">View a directory stack</a></li><li><a href="#pop-a-directory-from-the-stack">Pop a directory from the stack</a></li><li><a href="#swap-directories-on-the-stack">Swap directories on the stack</a></li><li><a href="#turn-a-mistaken-cd-into-a-pushd">Turn a mistaken cd into a pushd</a></li><li><a href="#go-deeper-into-the-stack">Go deeper into the stack</a></li></ul>
    </ul>
  </div>
  

  <p>
    Lets Suppose you need to perform a certain kind of work in all of these directories:&nbsp;
  </p>

  <ul>
    <li>/var/www/html</li><li>/etc/apache2</li><li>/etc/ssl/certs</li><li>~/Work/Projects/Web/src</li>
  </ul>

  <p>
    You might know about the "cd" command. But isn't it annoying if you need to use and retype a lot of Commands, like for example:
  </p>
<pre class=" language-bash"><code>$ cd ~/Work/Projects/Web/src
$ cd /var/www/html
$ cd /etc/apache2
$ cd ~/Work/Projects/Web/src
$ cd /etc/ssl/certs</code></pre>

  <p>
    But there is a much better and more efficient way. Take&nbsp;advantage of a shell feature called a directory stack.
  </p>

    <h3 id="directory-stack">
      Directory stack
    </h3>

  <p>
    You can manipulate the stack by performing two operations called <em>pushing</em> and <em>popping</em>.
  </p>

  <ul>
    <li>Pushing a&nbsp; directory adds it to the beginning of the list, which is traditionally called the top of the stack.</li><li>Popping removes the topmost directory from the stack.</li><li>Initially, the stack contains only your current directory, but you can add (push) and remove (pop) directories and rapidly cd among them.</li>
  </ul>

    <h3 id="push-a-directory-onto-the-stack">
      Push a directory onto the stack
    </h3>

  <p>
    The command pushd (short for “push directory”) does all of the following:
  </p>

  <ol>
    <li>Adds a given directory to the top of the stack</li><li>Performs a cd to that directory</li><li>Prints the stack from top to bottom for your reference</li>
  </ol>

  <p>
    Let's build a directory stack of four directories, pushing them onto the stack one at a time:
  </p>
<pre class=" language-bash"><code>$ pwd
/home/john/Work/Projects/Web/src
$ pushd /var/www/html
/var/www/html ~/Work/Projects/Web/src
$ pushd /etc/apache2
/etc/apache2 /var/www/html ~/Work/Projects/Web/src
$ pushd /etc/ssl/certs
/etc/ssl/certs /etc/apache2 /var/www/html
~/Work/Projects/Web/src
$ pwd
/etc/ssl/certs</code></pre>

  <p>
    The shell prints the stack after each pushd operation. The current directory is the leftmost (top) directory.
  </p>

    <h3 id="view-a-directory-stack">
      View a directory stack
    </h3>

  <p>
    Print a shell’s directory stack with the dirs command. It does not modify the stack:
  </p>
<pre class=" language-bash"><code>$ dirs -p
/etc/ssl/certs
/etc/apache2
/var/www/html
~/Work/Projects/Web/src</code></pre>

  <p>
    you can leave out the -p if you don't want to have each of them at it's own line. Or you can pass a -v to see them numbered:
  </p>
<pre class=" language-bash"><code>$ dirs -v
0 /etc/ssl/certs
1 /etc/apache2
2 /var/www/html
3 ~/Work/Projects/Web/src
</code></pre>

    <h3 id="pop-a-directory-from-the-stack">
      Pop a directory from the stack
    </h3>

  <p>
    The popd command (“pop directory”) is the reverse of pushd. It does all of the following:
  </p>

  <ol>
    <li>Removes one directory from the top of the stack</li><li>Performs a cd to the new top directory</li><li>Prints the stack from top to bottom for your reference<br></li>
  </ol>

  <p>
    For example, if your stack has four directories:
  </p>
<pre class=" language-bash"><code>$ dirs
/etc/ssl/certs /etc/apache2 /var/www/html
~/Work/Projects/Web/src
</code></pre>

  <p>
    then repeatedly running popd will traverse these directories from top to bottom:
  </p>
<pre class=" language-bash"><code>$ popd
/etc/apache2 /var/www/html ~/Work/Projects/Web/src
$ popd
/var/www/html ~/Work/Projects/Web/src
$ popd
~/Work/Projects/Web/src
$ popd
bash: popd: directory stack empty
$ pwd
~/Work/Projects/Web/src</code></pre>

    <h3 id="swap-directories-on-the-stack">
      Swap directories on the stack
    </h3>

  <p>
    Now that you can build and empty the directory stack, let’s focus on practical use cases. <em>pushd</em> with no arguments swaps the top two directories in the stack and navigates to the new top directory. Let’s jump between /etc/apache2 and your work directory several times by simply running <em>pushd</em>. See how the third directory /var/www/html remains in the stack as the first two directories swap positions:
  </p>
<pre class=" language-bash"><code>$ dirs
/etc/apache2 ~/Work/Projects/Web/src /var/www/html
$ pushd
~/Work/Projects/Web/src /etc/apache2 /var/www/html
$ pushd
/etc/apache2 ~/Work/Projects/Web/src /var/www/html
$ pushd
~/Work/Projects/Web/src /etc/apache2 /var/www/html</code></pre>

  <p>
    <em>pushd</em> behaves similarly to the <em>cd -</em> command, toggling between two directories, but it does not have the limitation of remembering just one directory.
  </p>

    <h3 id="turn-a-mistaken-cd-into-a-pushd">
      Turn a mistaken cd into a pushd
    </h3>

  <p>
    Suppose you are jumping among several directories with pushd and you accidentally run cd instead and lose a directory:
  </p>
<pre class=" language-bash"><code>$ dirs
~/Work/Projects/Web/src /var/www/html /etc/apache2
$ cd /etc/ssl/certs
$ dirs
/etc/ssl/certs /var/www/html /etc/apache2</code></pre>

  <p>
    Oops, the accidental cd command replaced ~/Work/Projects/Web/src in the stack with /etc/ssl/certs. But don’t worry. You can add the missing directory back to the stack without typing its long path. Just run <em>pushd</em> twice, once with a dash argument and once without:
  </p>
<pre class=" language-bash"><code>$ pushd -
~/Work/Projects/Web/src /etc/ssl/certs /var/www/html
/etc/apache2
$ pushd
/etc/ssl/certs ~/Work/Projects/Web/src /var/www/html
/etc/apache2</code></pre>

  <p>
    Why this works:
  </p>

  <ul>
    <li>The first pushd returns to your shell’s previous directory, ~/Work/Projects/Web/src, and pushes it onto the stack. pushd, like cd, accepts a dash as an argument to mean “go back to my previous directory.”</li><li>The second pushd command swaps the top two directories, bringing you back to /etc/ssl/certs. The end result is that you’ve restored ~/Work/Projects/Web/src to the second position in the stack, exactly where it would have been if you hadn’t made your mistake.</li>
  </ul>

    <h3 id="go-deeper-into-the-stack">
      Go deeper into the stack
    </h3>

  <p>
    What if you want to cd between directories in the stack other than the top two? <em>pushd</em> and <em>popd</em> accept a positive or negative integer argument to operate further into the stack. The command:
  </p>
<pre class=" language-bash"><code>$ pushd +N</code></pre>

  <p>
    shifts N directories from the top of the stack to the bottom and then&nbsp;performs a cd to the new top directory. A negative argument (-N) shifts directories in the opposite direction, from the bottom to the top, before performing the cd.
  </p>
<pre class=" language-bash"><code>$ dirs
/etc/ssl/certs ~/Work/Projects/Web/src /var/www/html
/etc/apache2
$ pushd +1
~/Work/Projects/Web/src /var/www/html /etc/apache2
/etc/ssl/certs
$ pushd +2
/etc/apache2 /etc/ssl/certs ~/Work/Projects/Web/src
/var/www/html</code></pre>

  <p>
    In this manner, you can jump to any other directory in the stack with a simple command. If your stack is long, however, it may be difficult to judge a directory’s numeric position by eye. So, print the numeric position of each directory with dirs -v, as you did in “View a<br>directory stack”:
  </p>
<pre class=" language-bash"><code>$ dirs -v
0 /etc/apache2
1 /etc/ssl/certs
2 ~/Work/Projects/Web/src
3 /var/www/html</code></pre>

  <p>
    To shift /var/www/html to the top of the stack (and make it your current directory), run pushd +3. To jump to the directory at the bottom of the stack, run pushd -0&nbsp;(dash zero):
  </p>
<pre class=" language-bash"><code>$ dirs
/etc/apache2 /etc/ssl/certs ~/Work/Projects/Web/src
/var/www/html
$ pushd -0
/var/www/html /etc/apache2 /etc/ssl/certs
~/Work/Projects/Web/src</code></pre>

  <p>
    You also can remove directories from the stack beyond the top directory, using popd with a numeric argument. The command:
  </p>
<pre class=" language-bash"><code>$ popd +N</code></pre>

  <p>
    removes the directory in position N from the stack, counting down from the top. A negative argument (-N) counts up from the bottom of the stack instead. Counting begins at zero, so popd +1 removes the second directory from the top:
  </p>
<pre class=" language-bash"><code>$ dirs
/var/www/html /etc/apache2 /etc/ssl/certs
~/Work/Projects/Web/src
$ popd +1
/var/www/html /etc/ssl/certs ~/Work/Projects/Web/src
$ popd +2
/var/www/html /etc/ssl/certs</code></pre>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Microservices</title>
        <author>
            <name>Finecloud</name>
        </author>
        <link href="https://www.finecloud.ch/microservices.html"/>
        <id>https://www.finecloud.ch/microservices.html</id>
            <category term="software development"/>
            <category term="microservices"/>

        <updated>2023-10-28T14:08:02+02:00</updated>
            <summary>
                <![CDATA[
                    This Post is a summary of the famous Article about Microservices: https://martinfowler.com/articles/microservices.html The text discusses the concept of "Microservice Architecture," which is an approach to designing software applications as a suite of independently deployable services. It highlights that there is no precise definition but outlines&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                
  <p>
    This Post is a summary of the famous Article about Microservices: https://martinfowler.com/articles/microservices.html<br>
  </p>

    <h2 id="a-definition-of-this-new-architectural-termlessbrgreater">
      a definition of this new architectural term<br>
    </h2>

  <p>
    The text discusses the concept of "Microservice Architecture," which is an approach to designing software applications as a suite of independently deployable services. It highlights that there is no precise definition but outlines common characteristics such as organization around business capabilities, automated deployment, decentralized control of languages and data, and the use of lightweight communication mechanisms like HTTP. Microservices are contrasted with monolithic architecture, where applications are built as a single unit. The text emphasizes that microservices provide advantages like independent deployment, scalability, and modular structure, making it increasingly appealing for building enterprise applications. The microservice style is not claimed to be innovative but is considered beneficial for software development.<br>
  </p>

    <h2 id="componentization-via-services">
      Componentization via Services
    </h2>

  <p>
    The text discusses the evolution of component-based software development in the software industry. It highlights the distinction between libraries and services as components, with a focus on microservice architectures. The main point is that components, in this context, are units of software that are independently replaceable and upgradeable. The text also explains that services, as out-of-process components, offer advantages in terms of independent deployability and explicit component interfaces. However, it acknowledges that using services can have downsides, such as increased overhead for remote calls and challenges in changing the allocation of responsibilities between components. The text concludes by noting that services can consist of multiple processes that are developed and deployed together.
  </p>

    <h2 id="organized-around-business-capabilities">
      Organized around Business Capabilities
    </h2>

  <p>
    The text mentions how companies like comparethemarket.com organize themselves using cross-functional teams responsible for building and operating individual services. The text also touches upon Conway's Law, emphasizing that an organization's system design mirrors its communication structure.
  </p>

  <p>
    The key point is the contrast between the traditional approach of splitting teams based on technology layers (UI, server-side, database) and the microservices approach, which focuses on dividing services around business capabilities. Microservices encourage cross-functional teams with expertise in user experience, database, and project management. The text suggests that large monolithic applications can also benefit from modularization based on business capabilities but cautions against excessive complexity and recommends maintaining clear team boundaries, which is facilitated by the more explicit separation in service components.
  </p>

    <h2 id="products-not-projects">
      Products not Projects
    </h2>

  <p>
    The text discusses the difference in development approaches between traditional project-based models and the microservices approach. In the traditional model, software development is seen as a project with a defined end, after which it's handed over to a maintenance organization. Microservice proponents advocate for teams to own a product throughout its entire lifecycle, emphasizing the "you build, you run it" philosophy popularized by Amazon.<br><br>This approach encourages developers to be responsible for their software in production, fostering closer interaction with how it behaves and its users. The text highlights that this product-oriented mentality aligns with the focus on business capabilities, emphasizing an ongoing relationship where software continuously enhances business capabilities.<br><br>It also notes that while this approach can be applied to monolithic applications, the smaller granularity of services in microservices makes it easier to establish personal relationships between service developers and their users.
  </p>

    <h2 id="smart-endpoints-and-dumb-pipes">
      Smart endpoints and dumb pipes
    </h2>

  <p>
    The text starts by mentioning the traditional approach, exemplified by Enterprise Service Bus (ESB), where significant intelligence is embedded in the communication mechanism itself, allowing for sophisticated message routing, choreography, and transformation.<br><br>In contrast, the microservices community prefers a different approach: "smart endpoints and dumb pipes." Microservices are designed to be highly decoupled and cohesive, with each service owning its domain logic. These services act as filters, receiving requests, applying logic, and producing responses. They utilize simple RESTish protocols and emphasize two common protocols: HTTP request-response with resource APIs and lightweight messaging. The principles of the World Wide Web and Unix underlie these protocols.<br><br>The text also highlights that, in microservices, the infrastructure used for messaging is typically simple and serves as a message router only, with the intelligence residing in the end points. The key challenge in transitioning from a monolithic architecture to microservices is changing the communication pattern. The text advises against a naive conversion to remote procedure calls (RPC) as it can lead to inefficient and "chatty" communications, advocating for a coarser-grained approach instead.
  </p>

    <h2 id="decentralized-governance">
      Decentralized Governance
    </h2>

  <p>
    The text highlights the limitations of centralized governance, such as the tendency to standardize on single technology platforms. In contrast, microservices allow for a more flexible approach, enabling teams to choose the right tools and technologies for specific components.<br><br>Microservice teams focus on producing practical tools and sharing them with other developers, often following open-source practices. This approach encourages flexibility in solving similar problems while still valuing service contracts. The text mentions patterns like Tolerant Reader and Consumer-Driven Contracts that help service contracts evolve independently, with tools enabling automated contract verification during the build process.<br><br>Furthermore, the text discusses the "build it / run it" ethos popularized by Amazon, where development teams are responsible for operating the software they build, emphasizing the decentralization of responsibility. This approach, exemplified by companies like Netflix, fosters a focus on code quality and contrasts sharply with traditional centralized governance models.
  </p>

    <h2 id="decentralized-data-management">
      Decentralized Data Management
    </h2>

  <p>
    The text points out that decentralized data management leads to differences in the conceptual models of systems, particularly when integrating across a large enterprise. This divergence in views can even occur within applications, especially when they are divided into separate components, which can be understood using the concept of Bounded Context from Domain-Driven Design.
  </p>

  <p>
    Microservices further decentralize data storage decisions by allowing each service to manage its own database, known as Polyglot Persistence. This contrasts with the monolithic approach of a single logical database for persistent data.<br><br>In terms of data updates, traditional monolithic applications often use transactions to guarantee consistency when updating multiple resources. However, microservices prioritize transactionless coordination between services due to the challenges of implementing distributed transactions. This approach acknowledges that consistency may be eventual and addresses problems with compensating operations.<br><br>The text also highlights that managing inconsistencies aligns with business practices where businesses often tolerate a degree of inconsistency to respond quickly to demand, with the ability to reverse processes to address mistakes. This trade-off is considered worthwhile as long as the cost of fixing errors is lower than the cost of lost business under greater consistency.
  </p>

    <h2 id="infrastructure-automation">
      Infrastructure Automation
    </h2>

  <p>
    The text points out that teams building microservices often have experience with Continuous Delivery and Continuous Integration, both of which heavily rely on infrastructure automation.<br><br>The text highlights that infrastructure automation plays a crucial role in building confidence in software by running automated tests and automating deployment to different environments. It mentions that once the path to production for a monolithic application is automated, deploying more applications becomes less daunting. The goal of Continuous Delivery is to make deployment a routine and uneventful process.
  </p>

  <p>
    The text also acknowledges that while the deployment process may not differ significantly between monolithic applications and microservices, the operational landscape for each can be notably distinct, suggesting that infrastructure automation is key to managing microservices effectively in production.
  </p>

    <h2 id="design-for-failure">
      Design for failure
    </h2>

  <p>
    The text points out that using services as components means applications need to be resilient and capable of handling service failures. Unlike monolithic designs, microservices introduce complexity in managing failures gracefully. To address this, microservice teams place a strong emphasis on monitoring and detecting failures in real-time.<br><br>The text mentions Netflix's "Simian Army," which intentionally induces service and datacenter failures during the working day to test the application's resilience and monitoring capabilities. While monolithic architectures can also have sophisticated monitoring, it's less common.<br><br>Microservices require the ability to quickly detect and, if possible, automatically restore service. They rely on real-time monitoring, checking both architectural and business-relevant metrics. Semantic monitoring helps spot issues, especially in a microservices architecture where choreography and event collaboration can lead to emergent behavior, which may not always be desirable.<br><br>The text concludes that microservice teams expect to have sophisticated monitoring and logging setups for each individual service, including dashboards for status, operational and business metrics, and details on circuit breaker status, throughput, and latency. Transparency and quick detection of failures are critical in a microservices environment.
  </p>

    <h2 id="evolutionary-design">
      Evolutionary Design
    </h2>

  <p>
    Microservice practitioners often come from an evolutionary design background and view service decomposition as a tool to enable application developers to control changes without slowing down the development process.<br><br>The key principle behind microservices is the notion of independent replacement and upgradeability. This means looking for points in the application where components can be rewritten without affecting their collaborators. Some microservice groups take this a step further by expecting that many services will be replaced rather than evolved in the long term.<br><br>The text provides examples of applications that started as monoliths but evolved in a microservice direction. These microservices are particularly useful for adding temporary features or services that are discarded after a short period, such as specialized pages for sporting events.<br><br>It emphasizes the importance of modular design based on the pattern of change, where components that change together should be in the same module. Microservices allow for more granular release planning, as changes only require redeploying the specific service(s) that were modified. However, this introduces the challenge of ensuring that changes to one service do not break its consumers, and the text suggests that versioning should be a last resort, with services designed to be tolerant of changes in their suppliers.
  </p>

    <h2 id="are-microservices-the-future">
      Are Microservices the Future?
    </h2>

  <p>
    The concept of microservices is a promising architectural style for enterprise applications but the text emphasizes that it's still too early to make definitive judgments about its long-term impact. Several well-known companies, including Amazon, Netflix, The Guardian, and others, have adopted microservices. However, the text acknowledges that the full consequences of architectural decisions may take several years to become evident.<br><br>It highlights some challenges and potential concerns associated with microservices, such as the difficulty of defining service boundaries, the increased complexity in coordinating interface changes, and the risk of moving complexity from within a component to the connections between components. Additionally, the success of microservices can be influenced by team skill, and it remains to be seen how less skillful teams would fare with this approach.<br><br>The text suggests a reasonable argument of starting with a monolith and splitting it into microservices when necessary, while maintaining modularity from the beginning. It concludes with cautious optimism, acknowledging that the microservices style holds promise, but the ultimate outcomes will depend on how well it addresses these challenges in practice.
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>The Concept of API Contracts</title>
        <author>
            <name>Finecloud</name>
        </author>
        <link href="https://www.finecloud.ch/the-concept-of-api-contracts.html"/>
        <id>https://www.finecloud.ch/the-concept-of-api-contracts.html</id>
            <category term="software development"/>
            <category term="api"/>

        <updated>2023-09-23T13:52:33+02:00</updated>
            <summary>
                <![CDATA[
                    What is it about? We define an API contract as a formal agreement between a software provider and a consumer that abstractly communicates how to interact with each other. This contract defines how API providers and consumers interact, what data exchanges looks like, and how&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                
    <h2 id="what-is-it-about">
      What is it about?
    </h2>

  <p>
    We define an API contract as a formal agreement between a software provider and a consumer that abstractly communicates how to interact with each other. This contract defines how API providers and consumers interact, what data exchanges looks like, and how to communicate success and failure cases.<br><br>The provider and consumers do not have to share the same programming language, only the same API contracts. Lets imagine that we need to design a API for a Family Cash Card Web Application. Let’s assume that currently there's one contract between the Cash Card service and all services using it. Below is an example of that first API contract.
  </p>
<pre class=" language-json"><code>Request
  URI: /cashcards/{id}
  HTTP Verb: GET
  Body: None

Response:
  HTTP Status:
    200 OK if the user is authorized and the Cash Card was successfully retrieved
    403 UNAUTHORIZED if the user is unauthenticated or unauthorized
    404 NOT FOUND if the user is authenticated and authorized but the Cash Card cannot be found
  Response Body Type: JSON
  Example Response Body:
    {
      "id": 99,
      "amount": 123.45
    }</code></pre>

    <h2 id="why-are-api-contracts-important">
      Why Are API Contracts Important?
    </h2>

  <p>
    API contracts are important because they communicate the behavior of a REST API. They provide specific details about the data being serialized (or deserialized) for each command and parameter being exchanged. The API contracts are written in such a way that can be easily translated into API provider and consumer functionality, and corresponding automated tests.
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Terraform Tips and Tricks</title>
        <author>
            <name>Finecloud</name>
        </author>
        <link href="https://www.finecloud.ch/terraform-tips-and-tricks-2.html"/>
        <id>https://www.finecloud.ch/terraform-tips-and-tricks-2.html</id>
            <category term="terraform"/>
            <category term="iac"/>
            <category term="devops"/>

        <updated>2023-09-17T14:14:00+02:00</updated>
            <summary>
                <![CDATA[
                    module "zland" { source = "git::ssh//git@gitlab.com/zland/module.git" version = "1.0.5" servers = 3 } Module Output Values resource "aws_instance" "appserver" { #... instance = module.servers.instance_ids } Since the resources defined in a module are encapsulated, a calling module cannot access their attributes directly. Instead, the child&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                
  <p>
    <br>
  </p>

  <div class="post__toc">
    <h3>Table of contents</h3>
    <ul>
      <li><a href="#facts-about-remote-state">Facts about Remote State</a></li><li><a href="#remote-state-storage-support">Remote State Storage support</a></li><li><a href="#separate-environments">Separate Environments</a></li><li><a href="#use-modules">Use Modules</a></li><li><a href="#arguments-to-use-with-modules">Arguments to use with Modules</a></li><li><a href="#module-output-valueslessbrgreater">Module Output Values<br></a><ul><li><a href="#create-a-custom-module-example">Create a custom Module (Example)</a></li></ul></li><li><a href="#dont-repeat-yourself">Don’t Repeat Yourself</a></li><li><a href="#3-things-to-use-to-keep-it-dry">3 Things to Use to Keep It DRY</a><ul><li><a href="#conditional-example">Conditional Example</a><ul><li><a href="#the-create_bucketfalse-conditional">The create_bucket=false Conditional</a></li><li><a href="#the-create_buckettrue-conditional">The create_bucket=true Conditional</a></li></ul></li></ul></li><li><a href="#use-null_resource">Use null_resource</a><ul><li><a href="#example-null_resource">Example null_resource</a></li></ul></li><li><a href="#use-functions">Use Functions</a><ul><li><a href="#the-format-function">The format Function</a></li><li><a href="#the-matchkeys-function">The matchkeys Function</a></li><li><a href="#the-element-function">The element Function</a></li></ul></li><li><a href="#test-your-code">Test Your Code</a><ul><li><a href="#other-testing-tools">Other Testing Tools</a></li></ul></li>
    </ul>
  </div>
  

    <h2 id="facts-about-remote-state">
      Facts about Remote State
    </h2>

  <ul>
    <li>Remote state is not the default; you must specify which backend to use and configure it to be used.</li><li>Remote state can be used by multiple team members. Terraform will write state data to a remote data store that users with access can use so there aren’t multiple state files.</li><li>Remote state uses a backend, which is configured in your configuration’s root module.</li><li>Remote state allows you to share output values with other configurations. Those configurations can then consume the exposed outputs in additional configurations.</li>
  </ul>

    <h2 id="remote-state-storage-support">
      Remote State Storage support
    </h2>

  <ul>
    <li>Terraform Cloud</li><li>HashiCorp Consul</li><li>Amazon S3</li><li>Azure Blob Storage</li><li>Google Cloud Storage</li><li>Alibaba Cloud OSS</li><li>...and more</li>
  </ul>

    <h2 id="separate-environments">
      Separate Environments
    </h2>

  <ol>
    <li>It is good practice to separate your Terraform configurations per environment.</li><li>Separate environments help with code organization, as well as allowing for better and easier CI and automation integration.</li><li>Implementing a one-folder-per-environment pattern lets you copy and paste Terraform code from one folder to another. This, used with variables, allows you to quickly change only what is needed per environment.</li>
  </ol>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://www.finecloud.ch/media/posts/92/Screenshot-2023-09-15-at-21.25.59.png" height="620" width="1432" alt=""  sizes="100vw" srcset="https://www.finecloud.ch/media/posts/92/responsive/Screenshot-2023-09-15-at-21.25.59-xs.png 300w ,https://www.finecloud.ch/media/posts/92/responsive/Screenshot-2023-09-15-at-21.25.59-sm.png 480w ,https://www.finecloud.ch/media/posts/92/responsive/Screenshot-2023-09-15-at-21.25.59-md.png 768w ,https://www.finecloud.ch/media/posts/92/responsive/Screenshot-2023-09-15-at-21.25.59-lg.png 1024w ,https://www.finecloud.ch/media/posts/92/responsive/Screenshot-2023-09-15-at-21.25.59-xl.png 1360w ,https://www.finecloud.ch/media/posts/92/responsive/Screenshot-2023-09-15-at-21.25.59-2xl.png 1600w">
      
    </figure>

    <h2 id="use-modules">
      Use Modules
    </h2>

  <ol>
    <li>Modules are containers for multiple resources that are used together.</li><li>Every Terraform configuration contains at least one module.</li><li>Modules can call other modules. This lets you include a module’s resources in a configuration in a concise way.</li><li>Modules can be called multiple times, either in the same Terraform configuration or in separate ones. This allows for resource configurations to be packaged and reused.</li>
  </ol>

    <h2 id="arguments-to-use-with-modules">
      Arguments to use with Modules
    </h2>

  <ul>
    <li>source: This argument is mandatory for all modules.</li><li>version: This argument is recommended for modules from a registry.</li><li>meta-arguments: Arguments like for_each and count.</li><li>input variables: Most other arguments correspond to input variables.</li>
  </ul>
<pre class=" language-go"><code>module "zland" {
  source = "git::ssh//git@gitlab.com/zland/module.git"
  version = "1.0.5"
  servers = 3
}</code></pre>

    <h2 id="module-output-valueslessbrgreater">
      Module Output Values<br>
    </h2>
<pre class=" language-go"><code>resource "aws_instance" "appserver" {
    #...
    instance = module.servers.instance_ids
}</code></pre>

  <p>
    Since the resources defined in a module are encapsulated, a calling module cannot access their attributes directly. Instead, the child module can declare output values.
  </p>

    <h3 id="create-a-custom-module-example">
      Create a custom Module (Example)
    </h3>

  <p>
    Create those files in the folder modules/ec2:<br><br>main.tf
  </p>
<pre class=" language-go"><code>resource "aws_instance" "app_server" {
  ami           = "DUMMY_VALUE_AMI"
  instance_type = "t3.micro"
  subnet_id     = "DUMMY_VALUE_SUBNET_ID"
  tags = {
    Name = "WayneCorp"
  }
}
</code></pre>

  <p>
    outputs.tf
  </p>
<pre class=" language-go"><code>output "instance_id" {
  description = "ID of the EC2 instance"
  value       = aws_instance.app_server.id
}

output "instance_public_ip" {
  description = "Public IP address of the EC2 instance"
  value       = aws_instance.app_server.public_ip
}</code></pre>

  <p>
    to use your module, add this snippet at the end of you existing ec2.tf file:<br><br>existing ec2.tf:
  </p>
<pre class=" language-go"><code>terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~&gt; 3.27"
    }
  }
  required_version = "&gt;= 0.14.9"
}
provider "aws" {
  profile = "default"
  region  = "us-east-1"
}</code></pre>

  <p>
    new content to add:
  </p>
<pre class=" language-go"><code>module "ec2-module" {
  source = "./modules/ec2/"
}
</code></pre>

  <p>
    now you can run a <code>terraform fmt </code>to format your code.<br><br>Now run a <code>terraform init</code>&nbsp;to initialize your terraform backend<br><br>run a <code>terraform validate</code>&nbsp;to make sure syntax is correct<br><br>finally run a <code>terraform plan</code>&nbsp;and if your happy with the output a terraform apply<br><br>To check your state you can now run <code>terraform show</code>&nbsp;or <code>aws ec2 describe-instances</code>
  </p>

    <h2 id="dont-repeat-yourself">
      Don’t Repeat Yourself
    </h2>

  <ul>
    <li>DRY is a principle that promotes modularization, abstraction, and code reuse and discourages repetition.</li><li>    This principle states that “every piece of knowledge must have a single, unambiguous, authoritative representation within a system”.</li><li>    This principle can be applied to not only programming, but to database schemas, test plans, the build system, and even documentation.</li><li>    If applied successfully, a modification of a single piece of the system will not require a change in other logic or unrelated elements of the system.</li>
  </ul>

  <p>
    Not Keeping It DRY looks like this:
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://www.finecloud.ch/media/posts/92/Screenshot-2023-09-18-at-13.40.41.png" height="482" width="1958" alt=""  sizes="100vw" srcset="https://www.finecloud.ch/media/posts/92/responsive/Screenshot-2023-09-18-at-13.40.41-xs.png 300w ,https://www.finecloud.ch/media/posts/92/responsive/Screenshot-2023-09-18-at-13.40.41-sm.png 480w ,https://www.finecloud.ch/media/posts/92/responsive/Screenshot-2023-09-18-at-13.40.41-md.png 768w ,https://www.finecloud.ch/media/posts/92/responsive/Screenshot-2023-09-18-at-13.40.41-lg.png 1024w ,https://www.finecloud.ch/media/posts/92/responsive/Screenshot-2023-09-18-at-13.40.41-xl.png 1360w ,https://www.finecloud.ch/media/posts/92/responsive/Screenshot-2023-09-18-at-13.40.41-2xl.png 1600w">
      
    </figure>

  <p>
    Keeping it DRY on the oder hand look like this:
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://www.finecloud.ch/media/posts/92/Screenshot-2023-09-18-at-13.43.16.png" height="590" width="2216" alt=""  sizes="100vw" srcset="https://www.finecloud.ch/media/posts/92/responsive/Screenshot-2023-09-18-at-13.43.16-xs.png 300w ,https://www.finecloud.ch/media/posts/92/responsive/Screenshot-2023-09-18-at-13.43.16-sm.png 480w ,https://www.finecloud.ch/media/posts/92/responsive/Screenshot-2023-09-18-at-13.43.16-md.png 768w ,https://www.finecloud.ch/media/posts/92/responsive/Screenshot-2023-09-18-at-13.43.16-lg.png 1024w ,https://www.finecloud.ch/media/posts/92/responsive/Screenshot-2023-09-18-at-13.43.16-xl.png 1360w ,https://www.finecloud.ch/media/posts/92/responsive/Screenshot-2023-09-18-at-13.43.16-2xl.png 1600w">
      
    </figure>

  <p>
    the configurations are symlinked here, this allows us to share the same configurations between those environments
  </p>

    <h2 id="3-things-to-use-to-keep-it-dry">
      3 Things to Use to Keep It DRY
    </h2>

  <ul>
    <li>Terraform supports conditionals through the syntax of a ternary operator.</li><li>    The most common use case for conditionals is to create a conditional resource based on an input variable and the meta-parameter count.</li>
  </ul>

    <h3 id="conditional-example">
      Conditional Example
    </h3>
<pre class=" language-go"><code>locals {
  make_bucket = "${var.create_bucket == "true" ? True : false}"
}
resource "google_storage_bucket” “twinkiebucket" {
  count = "${local.make_bucket ? 1 : 0}"
  name = "${var.bucket_name}"
  project = "${var.project_name}"
}</code></pre>

    <h4 id="the-create_bucketfalse-conditional">
      The create_bucket=false Conditional
    </h4>

  <p>
    output:
  </p>

  <p>
    —&gt; test-bucket terraform plan -var=‘create_bucket=false’<br>Refreshing Terraform state in-memory prior to plan...<br>The refreshed state will be used to calculate this plan, but will not be Persisted to local or remote state storage.
  </p>

  <p>
    No changes. Infrastructure is up-to-date.<br><br>This means that Terraform did not detect any differences between your configuration and real physical resources that exist. As a result, no actions need to be performed.
  </p>

    <h4 id="the-create_buckettrue-conditional">
      The create_bucket=true Conditional
    </h4>

  <p>
    output:
  </p>

  <p>
    —&gt; test-bucket terraform plan -var=‘create_bucket=true’<br>Refreshing Terraform state in-memory prior to plan...<br>The refreshed state will be used to calculate this plan, but will not be Persisted to local or remote state storage.<br><br>An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols:<br><br>+ create<br>Terraform will perform the following actions:<br><br>+ google_storage_bucket.twinkiebucket ...
  </p>

    <h2 id="use-null_resource">
      Use null_resource
    </h2>

  <ul>
    <li>The null_resource is useful when you need to do something that is not directly associated with the lifecycle of an actual resource.</li><li>    Within a null_resource, you can configure provisioners to run scripts to do pretty much whatever you want.</li><li>    Just like with provisioners, it is a good idea to use null_resource sparingly since it adds to the complexity of your Terraform usage.</li><li>    Make sure, when you do use it, that you vet the scripts being called thoroughly.</li>
  </ul>

    <h3 id="example-null_resource">
      Example null_resource
    </h3>
<pre class=" language-go"><code>resource "aws_instance" "prod_cluster" {
    count = 4
    #...
}
resource "null_resource" "prod_cluster" {
    triggers = {
        cluster_instance_ids = join("," aws_instance.prod_cluster.*.id)
    }
    connection {
        host = element(aws_instance.prod_cluster.*.public_ip, 0)
    }
    provisioner "remote-exec" {
        inline = [
            "prod_cluster.sh ${join(" ", aws_instance.prod_cluster.*.private_ip)}",
        ]
    }
}
</code></pre>

  <p class="msg msg--highlight">
    Actions that are done inside a null_resource are not managed by Terraform. If you decide to call a command to create resources in your null_resource, Terraform will not know about the resource creation, and therefore can’t manage its lifecycle and state.
  </p>

  <p>
    you could for example add those lines to your main.tf:
  </p>
<pre class=" language-go"><code>resource "null_resource" "ec2_status" {
  provisioner "local-exec" {
    command = "./scripts/health.sh"
  }
}
</code></pre>

  <p>
    and add the health.sh script to your repository:
  </p>
<pre class=" language-bash"><code>#!/bin/bash
echo "   -------------------------------- "
echo "  --&gt; Fetching Instance status."
sleep 25
instance_id=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=TheFastestManAlive" "Name=instance-state-name,Values=running" --query 'Reservations[*].Instances[*].InstanceId' --output text)
size=${#instance_id}
echo "  --&gt; Instance ID: $instance_id"
sleep 2
instance_state=$(aws ec2 describe-instance-status --instance-ids $instance_id --query 'InstanceStatuses[*].InstanceState.Name' --output text)
size=${#instance_state}
echo "  --&gt; Instance Status: $instance_state"
sleep 2
instance_zone=$(aws ec2 describe-instance-status --instance-ids $instance_id --query 'InstanceStatuses[*].AvailabilityZone' --output text)
size=${#instance_zone}
echo "  --&gt; Availability Zone: $instance_zone"
sleep 2
fetch_instance_health=$(aws ec2 describe-instance-status --instance-ids $instance_id --query 'InstanceStatuses[*].InstanceStatus.Status' --output text)
echo "  --&gt; Instance health check : $fetch_instance_health"
echo "  -------------------------------------------"</code></pre>

  <p>
    do run some trivial heal check after the deployment of our EC2 instance.
  </p>

    <h2 id="use-functions">
      Use Functions
    </h2>

  <ul>
    <li>Terraform has built-in interpolation functions that allow you to use interpolation syntax embedded within strings to interpolate other values.</li><li>Interpolationfunctionsarecalledwiththesyntax${name(arg, arg2, ...)}.</li><li>The interpolation syntax allows you to call a large list of built-in functions.</li>
  </ul>

    <h3 id="the-format-function">
      The format Function
    </h3>
<pre class=" language-bash"><code>#format.tf
locals {
hostname = "${format("%s-%s-%s-%s-%04d-%s", var.region, var.env, var.app,
var.type, var.cluster_id, var.id)}" }</code></pre>

  <p>
    this Terraform code defines a local variable named <em>hostname</em>&nbsp;using the locals block. This variable is computed using the `format` function and a string template. Let's break down the components of this function:<br><br>1. `${format(...)}:` This part of the code is using Terraform's interpolation syntax `${...}` to execute the `format` function. The `format` function is used to create formatted strings by substituting values into placeholders within a template string.<br><br>2. `"${format("%s-%s-%s-%s-%04d-%s", var.region, var.env, var.app, var.type, var.cluster_id, var.id)}"`: This is the template string used in the `format` function. It consists of several placeholders, each represented by `%s` or `%04d`, which are replaced by the values provided after the template string.
  </p>

  <ul>
    <li>`%s`: This is a placeholder for a string value.</li><li>`%04d`: This is a placeholder for a decimal integer value, formatted with leading zeros to ensure a total width of 4 characters.</li>
  </ul>

  <p>
    The values to be substituted into these placeholders come from various Terraform variables:
  </p>

  <ul>
    <li>`var.region`: This variable is expected to contain a string representing a region.</li><li>`var.env`: This variable is expected to contain a string representing an environment.</li><li>`var.app`: This variable is expected to contain a string representing an application name.</li><li>`var.type`: This variable is expected to contain a string representing a type.</li><li>`var.cluster_id`: This variable is expected to contain a numeric cluster identifier.</li><li>`var.id`: This variable is expected to contain a string or value that is used in the formatted hostname.</li>
  </ul>

  <p>
    The `format` function combines these values using the specified template to generate a formatted hostname. The resulting hostname will be a string that includes the region, environment, application, type, cluster identifier (with leading zeros if necessary), and the additional identifier provided by `var.id`.<br><br>For example, if you have the following values for your variables:
  </p>

  <ul>
    <li>`var.region` = "us-west"</li><li>`var.env` = "prod"</li><li>`var.app` = "web"</li><li>`var.type` = "frontend"</li><li>`var.cluster_id` = 42</li><li>`var.id` = "abc123"</li>
  </ul>

  <p>
    The `hostname` variable will be computed as follows:<br>
  </p>
<pre class=" language-bash"><code>us-west-prod-web-frontend-0042-abc123</code></pre>

  <p>
    This computed hostname can then be used in your Terraform configuration as needed, such as for provisioning cloud resources with this specific hostname format.
  </p>

    <h3 id="the-matchkeys-function">
      The matchkeys Function
    </h3>

  <p>
    matchkeys constructs a new list by taking a subset of elements from one list whose indexes match the corresponding indexes of values in another list.<br><br>matchkeys identifies the indexes in keyslist that are equal to elements of searchset, and then constructs a new list by taking those same indexes from valueslist. Both valueslist and keyslist must be the same length.<br><br>The ordering of the values in valueslist is preserved in the result.
  </p>
<pre class=" language-bash"><code>#matchkeys.tf
instances = [ "${matchkeys(
  google_compute_instance.compute_instance.*.self_link,
  google_compute_instance.compute_instance.*.zone,
  data.google_compute_zones.available.names[0])
}" ]</code></pre>

    <h3 id="the-element-function">
      The element Function
    </h3>

  <p>
    The `element` function in Terraform is primarily used for accessing elements within a list or an array. It's a versatile function that can be used for various purposes, including:
  </p>

  <ol>
    <li>Retrieving Values: You can use `element` to retrieve specific values from a list or array. For example, you might use it to access the nth element of a list.</li><li><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: 1em; font-weight: var(--font-weight-normal);">Looping and Iteration: When combined with other Terraform constructs like `count` or `for_each`, `element` can be used to iterate over a list or array, applying the same resource configuration or operation to each element.</span></li><li>Dynamic Resource Creation: In Terraform, you can use `element` to dynamically create multiple instances of a resource by specifying different configurations for each instance based on the elements of a list or array.</li><li><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: 1em; font-weight: var(--font-weight-normal);">Conditional Behavior: It can be used to conditionally set values or attributes in resources or variables based on the index of an element in a list.</span></li>
  </ol>

  <p>
    Here's an example of how you might use the `element` function in a Terraform configuration:
  </p>
<pre class=" language-bash"><code>variable "server_names" {
  type    = list(string)
  default = ["web-server-1", "web-server-2", "web-server-3"]
}

resource "aws_instance" "example" {
  count = length(var.server_names)
  ami   = "ami-12345678"
  instance_type = "t2.micro"
  tags = {
    Name = element(var.server_names, count.index)
  }
}
</code></pre>

  <p>
    In this example, the `element` function is used to assign a unique name tag to each AWS EC2 instance being created based on the elements of the "server_names" list. It demonstrates how `element` can be used for dynamic resource creation and conditional behavior.<br><br>Overall, the `element` function is a fundamental tool in Terraform for working with lists and arrays, enabling you to make your configurations more dynamic and flexible.
  </p>

    <h2 id="test-your-code">
      Test Your Code
    </h2>

  <ul>
    <li>Testing code leads to greater confidence that the code will perform as expected.</li><li>Terraform has built-in tools to help test your code before deployment.</li><li>Due to Terraform’s usefulness and popularity, there are many tools which expand upon the built-in tools.</li>
  </ul>

  <p>
    there are a few built-in commands to test your TF code:
  </p>

  <ol>
    <li>terraform fmt</li><li>terraform init</li><li>terraform validate</li><li>terraform plan</li>
  </ol>

    <h3 id="other-testing-tools">
      Other Testing Tools
    </h3>

  <ul>
    <li>Terratest:&nbsp;A great, comprehensive tool by Gruntwork. This tool does not do unit testing.</li><li>Kitchen-Terraform:&nbsp;Spins up, tests, and spins down various Terraform resources.</li><li>Terraform-compliance:&nbsp;A simple tool for testing and enforcing Terraform compliance rules.</li>
  </ul>
            ]]>
        </content>
    </entry>
</feed>
